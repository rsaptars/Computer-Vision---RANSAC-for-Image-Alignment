{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version required for this code: opencv-python==3.4.2.16\n",
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "#Important Threshold Parameter\n",
    "BEST_MATCH = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that aligns the two images using RANSAC\n",
    "def image_alignment_func(image_1, image_2):\n",
    "\n",
    "    # Conversion of images to grayscale\n",
    "    gray_image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "    # Detect SIFT features and compute descriptors for the images\n",
    "    desc = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints_image_1, descriptors_image_1 = desc.detectAndCompute(gray_image_1, None)\n",
    "    keypoints_image_2, descriptors_image_2 = desc.detectAndCompute(gray_image_2, None)\n",
    "    \n",
    "    # Convert keypoints to float values for further processing\n",
    "    keypoints_image_1 = np.float32([i.pt for i in keypoints_image_1])\n",
    "    keypoints_image_2 = np.float32([i.pt for i in keypoints_image_2])\n",
    "    \n",
    "    # Match features for the images\n",
    "    matcher_for_image = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "    matches_found = matcher_for_image.knnMatch(descriptors_image_1, descriptors_image_2, 2)\n",
    "\n",
    "    best_matches = []\n",
    "    for val in matches_found:\n",
    "        if len(val) == 2 and val[0].distance < val[1].distance * BEST_MATCH:\n",
    "            best_matches.append((val[0].trainIdx, val[0].queryIdx))\n",
    "\n",
    "    # Get the location of the top matches\n",
    "    points_image_1 = np.zeros((len(best_matches), 2), dtype=np.float32)\n",
    "    points_image_2 = np.zeros((len(best_matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(best_matches):\n",
    "        points_image_1 = np.float32([keypoints_image_1[i] for (_,i) in best_matches])\n",
    "        points_image_2 = np.float32([keypoints_image_2[i] for (i,_) in best_matches])\n",
    "    \n",
    "    # Find homography\n",
    "    homo_index, mask = cv2.findHomography(points_image_1, points_image_2, cv2.RANSAC, 4.0)\n",
    "    \n",
    "    # Warping the images using homography factor\n",
    "    width = image_1.shape[1] + image_2.shape[1];\n",
    "    image_1_registered = cv2.warpPerspective(image_1, homo_index, (width, image_1.shape[0]))\n",
    "    image_1_registered[0:image_2.shape[0], 0:image_2.shape[1]] = image_2\n",
    "    \n",
    "    # Draw the matches for the 2 images\n",
    "    height_1 = image_1.shape[0]\n",
    "    height_2 = image_2.shape[0]\n",
    "    width_1 = image_1.shape[1]\n",
    "    width_2 = image_2.shape[1]\n",
    "    \n",
    "    matches = np.zeros((max(height_1, height_2), width_1 + width_2, 3), dtype=\"uint8\")\n",
    "    matches[0:height_1, 0:width_1] = image_1\n",
    "    matches[0:height_2, width_1:] = image_2\n",
    "    \n",
    "    for ((trainIdx, queryIdx), s) in zip(best_matches, mask):\n",
    "        if s == 1:\n",
    "            point_1 = (int(keypoints_image_1[queryIdx][0]), int(keypoints_image_1[queryIdx][1]))\n",
    "            point_2 = (int(keypoints_image_2[trainIdx][0]) + width_1, int(keypoints_image_2[trainIdx][1]))\n",
    "            cv2.line(matches, point_1, point_2, (255, 0, 255), 1)\n",
    "\n",
    "    cv2.imwrite(\"Match.jpg\",matches)\n",
    "    \n",
    "    return image_1_registered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference image (Image 1) :  input1.png\n",
      "Image to be aligned (Image 2) :  input2.png\n",
      "RANSAC for image alignment:\n",
      "Aligned Image saved to the disk:  aligned.jpg\n"
     ]
    }
   ],
   "source": [
    "# Reference image (Image 1)\n",
    "image_1_fname = \"input1.png\"\n",
    "print(\"Reference image (Image 1) : \", image_1_fname)\n",
    "image_reference = cv2.imread(image_1_fname, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Image to be aligned (Image 2)\n",
    "image_2_fname = \"input2.png\"\n",
    "print(\"Image to be aligned (Image 2) : \", image_2_fname);  \n",
    "image_align = cv2.imread(image_2_fname, cv2.IMREAD_COLOR)\n",
    "  \n",
    "print(\"RANSAC for image alignment:\")\n",
    "\n",
    "# Registered image will be stored in image_registered \n",
    "# Estimated homography will be stored in est_homography\n",
    "image_registered = image_alignment_func(image_align, image_reference)\n",
    "  \n",
    "# Save the aligned image to the disk\n",
    "result_filename = \"aligned.jpg\"\n",
    "cv2.imwrite(result_filename, image_registered)\n",
    "print(\"Aligned Image saved to the disk: \", result_filename);   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
